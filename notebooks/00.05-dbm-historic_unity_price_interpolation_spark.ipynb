{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "# #%% md\n",
    "# Shipper-Brand Relation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43393a88d8db239d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esse notebook tem por objetivo fazer a relação de empresas que importam somente uma marca, facilitando a relação importador-"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee3547a85086aa1f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2247626460e056ff",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:47:45.612990700Z",
     "start_time": "2024-02-20T18:47:45.439438200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the modules needed\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from src.data.dremio_utils import *\n",
    "# Data Handling\n",
    "from dotenv import dotenv_values \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"attributes_dict\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:48:10.243514500Z",
     "start_time": "2024-02-20T18:47:45.458975200Z"
    }
   },
   "id": "9f4b6b3ba58fedc9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "bds = BaseDremioService(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:48:10.297359500Z",
     "start_time": "2024-02-20T18:48:10.248037500Z"
    }
   },
   "id": "b958cb0ee3f1a4f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Getting Merged Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a857a09bd42d6f3"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "grouped_data = spark.read.parquet(\"../data/processed/average_unity_price_historic.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:48:15.140973700Z",
     "start_time": "2024-02-20T18:48:10.279843Z"
    }
   },
   "id": "890265008d0fe9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "grouped_data = grouped_data.groupBy(['ncm', 'importador_uf', 'importador_municipio', 'urf', 'id_pais_origem', 'ano', 'semestre']).avg('avg_valor_item')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:48:15.388550500Z",
     "start_time": "2024-02-20T18:48:15.145981700Z"
    }
   },
   "id": "42996bef6833a929"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Gabarito de datas e combinações "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2da18f3ad6a0b876"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------+\n",
      "|      ncm|       importador_uf|importador_municipio|                 urf|id_pais_origem|\n",
      "+---------+--------------------+--------------------+--------------------+--------------+\n",
      "|     null|              TOLEDO|                  PR|     PORTO DE SANTOS|    BANGLADESH|\n",
      "|2023000.0|             GOIANIA|                  GO|IRF - PORTO DE SUAPE|     AUSTRÁLIA|\n",
      "|3055310.0|CABO DE SANTO AGO...|                  PE|              ITAJAI|       NORUEGA|\n",
      "|4071900.0|               BAURU|                  SP|AEROPORTO INTERNA...|ESTADOS UNIDOS|\n",
      "|6029029.0|                  SP|            HOLAMBRA|AEROPORTO INTERNA...|       HOLANDA|\n",
      "+---------+--------------------+--------------------+--------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "unique_combinations = grouped_data.select('ncm', 'importador_uf', 'importador_municipio', 'urf', 'id_pais_origem').distinct()\n",
    "unique_combinations.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:58:17.085816300Z",
     "start_time": "2024-02-20T18:48:15.389017Z"
    }
   },
   "id": "8e9ea8a69fd5f2fc"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| ano|semestre|\n",
      "+----+--------+\n",
      "|2018|       1|\n",
      "|2018|       2|\n",
      "|2019|       1|\n",
      "|2019|       2|\n",
      "|2020|       1|\n",
      "+----+--------+\n"
     ]
    }
   ],
   "source": [
    "anos_data = [{\"ano\": 2018}, {\"ano\": 2019}, {\"ano\": 2020}, {\"ano\": 2021}, {\"ano\": 2022}, {\"ano\": 2023}]\n",
    "anos_schema =StructType([StructField(\"ano\", IntegerType())])\n",
    " \n",
    "years_df = spark.createDataFrame(data=anos_data, schema=anos_schema)\n",
    "\n",
    "semestres_data = [{\"semestre\": 1}, {\"semestre\": 2}]\n",
    "semestres_schema =StructType([StructField(\"semestre\", IntegerType())])\n",
    "\n",
    "semesters_df = spark.createDataFrame(data=semestres_data, schema=semestres_schema)\n",
    "\n",
    "gabarito_datas = years_df.crossJoin(semesters_df)\n",
    "gabarito_datas.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:58:19.335692700Z",
     "start_time": "2024-02-20T18:58:17.083819500Z"
    }
   },
   "id": "443963ec9c879ca6"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+---------------+--------------+----+--------+\n",
      "| ncm|importador_uf|importador_municipio|            urf|id_pais_origem| ano|semestre|\n",
      "+----+-------------+--------------------+---------------+--------------+----+--------+\n",
      "|null|       TOLEDO|                  PR|PORTO DE SANTOS|    BANGLADESH|2018|       1|\n",
      "|null|       TOLEDO|                  PR|PORTO DE SANTOS|    BANGLADESH|2018|       2|\n",
      "|null|       TOLEDO|                  PR|PORTO DE SANTOS|    BANGLADESH|2019|       1|\n",
      "|null|       TOLEDO|                  PR|PORTO DE SANTOS|    BANGLADESH|2019|       2|\n",
      "|null|       TOLEDO|                  PR|PORTO DE SANTOS|    BANGLADESH|2020|       1|\n",
      "+----+-------------+--------------------+---------------+--------------+----+--------+\n"
     ]
    }
   ],
   "source": [
    "gabarito_comb = unique_combinations.crossJoin(gabarito_datas)\n",
    "gabarito_comb.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:06:50.503251Z",
     "start_time": "2024-02-20T18:58:19.326099200Z"
    }
   },
   "id": "c270ff65c1823768"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o88.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 11.0 failed 1 times, most recent failure: Lost task 3.0 in stage 11.0 (TID 834, 192.168.0.7, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m df_2b_filled \u001B[38;5;241m=\u001B[39m gabarito_comb\u001B[38;5;241m.\u001B[39mjoin(grouped_data, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mncm\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportador_uf\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportador_municipio\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124murf\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid_pais_origem\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mano\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msemestre\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mdf_2b_filled\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Spark\\python\\pyspark\\sql\\dataframe.py:1353\u001B[0m, in \u001B[0;36mDataFrame.head\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1351\u001B[0m     rs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   1352\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m rs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m rs \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1353\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Spark\\python\\pyspark\\sql\\dataframe.py:639\u001B[0m, in \u001B[0;36mDataFrame.take\u001B[1;34m(self, num)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;129m@ignore_unicode_prefix\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;129m@since\u001B[39m(\u001B[38;5;241m1.3\u001B[39m)\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtake\u001B[39m(\u001B[38;5;28mself\u001B[39m, num):\n\u001B[0;32m    634\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001B[39;00m\n\u001B[0;32m    635\u001B[0m \n\u001B[0;32m    636\u001B[0m \u001B[38;5;124;03m    >>> df.take(2)\u001B[39;00m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;124;03m    [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\u001B[39;00m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Spark\\python\\pyspark\\sql\\dataframe.py:596\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    590\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[0;32m    591\u001B[0m \n\u001B[0;32m    592\u001B[0m \u001B[38;5;124;03m>>> df.collect()\u001B[39;00m\n\u001B[0;32m    593\u001B[0m \u001B[38;5;124;03m[Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\u001B[39;00m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc) \u001B[38;5;28;01mas\u001B[39;00m css:\n\u001B[1;32m--> 596\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    597\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001B[1;32m~\\test_environment\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py:128\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 128\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    130\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\test_environment\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o88.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 11.0 failed 1 times, most recent failure: Lost task 3.0 in stage 11.0 (TID 834, 192.168.0.7, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n"
     ]
    }
   ],
   "source": [
    "df_2b_filled = gabarito_comb.join(grouped_data, ['ncm', 'importador_uf', 'importador_municipio', 'urf', 'id_pais_origem', 'ano', 'semestre'], \"left\")\n",
    "df_2b_filled.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:21:48.563825900Z",
     "start_time": "2024-02-20T19:06:50.504250900Z"
    }
   },
   "id": "bfc7c07e1f5179ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2b_filled = df_2b_filled.withColumnRenamed(\"avg(avg_valor_item)\", \"avg_valor_item\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.556808400Z"
    }
   },
   "id": "2a40329c920dbdaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2b_filled.write.parquet(\"../data/processed/data_2b_filled\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.559827900Z"
    }
   },
   "id": "666c0d377be86c49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del df_2b_filled, gabarito_comb, grouped_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.564829300Z"
    }
   },
   "id": "1bc75d425bd81984"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Caminho para a pasta contendo os arquivos Parquet\n",
    "caminho = '../data/processed/data_2b_filled/'\n",
    "\n",
    "# Lista de todos os arquivos Parquet\n",
    "arquivos_parquet = [arq for arq in os.listdir(caminho) if arq.lower().endswith('.parquet')]\n",
    "\n",
    "concat_df = pd.DataFrame()\n",
    "with tqdm(desc=\"concatenating parquet\", total=len(arquivos_parquet)) as pbar:\n",
    "    for arquivo in arquivos_parquet:\n",
    "        df_aux = pd.read_parquet(caminho+arquivo)\n",
    "        concat_df = pd.concat([concat_df,df_aux])\n",
    "        pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:21:48.644633800Z",
     "start_time": "2024-02-20T19:21:48.569993200Z"
    }
   },
   "id": "840b689783d6d77a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Inference of the value for the first semesters of 2024"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3db62a2081e302a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Inference using linear interpolation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cf18a1cec0190c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "years_df = pd.DataFrame.from_dict({\"ano\": [2018, 2019, 2020, 2021, 2022, 2023, 2024]})\n",
    "semesters_df = pd.DataFrame.from_dict({\"semestre\": [1, 2]})\n",
    "gabarito_datas = years_df.join(semesters_df, how=\"cross\")\n",
    "gabarito_comb = unique_combinations.join(gabarito_datas, how=\"cross\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.572994800Z"
    }
   },
   "id": "b9b331a51addcfd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2b_infer = gabarito_comb.merge(df_filled, how=\"left\", on=['ncm', 'importador_uf', 'importador_municipio', 'urf', 'name_pt', 'ano', 'semestre'])\n",
    "df_2b_infer.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.578517500Z"
    }
   },
   "id": "b4b95d3b12807459"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2b_infer[\"avg_valor_item\"] = df_2b_infer[\"avg_valor_item\"].interpolate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.582511700Z"
    }
   },
   "id": "858fc6f1a6bc6654"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2b_infer[\"anosem\"] = df_2b_infer[\"ano\"].astype(str) + df_2b_infer[\"semestre\"].astype(str) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.584514600Z"
    }
   },
   "id": "5fb17ca1c18b74a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _, df in df_2b_infer.groupby(['ncm', 'importador_uf', 'importador_municipio', 'urf', 'name_pt']):\n",
    "    print(df[\"avg_valor_item\"].values)\n",
    "    df.plot(y=\"avg_valor_item\", x=\"anosem\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T19:21:48.587023400Z"
    }
   },
   "id": "74dccca8b519100f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Inference using linear interpolation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f3311143168a90b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
